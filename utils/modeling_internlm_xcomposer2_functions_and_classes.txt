class InternLMXComposer2ForCausalLM(InternLM2PreTrainedModel):
def __init__(self, config):
def _set_gradient_checkpointing(self, module, value):
def get_input_embeddings(self):
def set_input_embeddings(self, value):
def get_output_embeddings(self):
def set_output_embeddings(self, new_embeddings):
def set_decoder(self, decoder):
def get_decoder(self):
def encode_text(self, text, add_special_tokens):
def encode_img(self, image):
def img2emb(self, image):
def prompt_wrap(self, img_embeds, prompt):
def text2emb(self, text, add_special):
def interleav_wrap_chat(self, tokenizer, query, image, history, meta_instruction):
def interleav_wrap(self, img_list, text_list):
def mask_human_targets(self, input_ids, pure):
def forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict):
def prepare_inputs_for_generation(self, input_ids, past_key_values, attention_mask, inputs_embeds, im_mask):
def _reorder_cache(past_key_values, beam_idx):
def build_inputs(self, tokenizer, query, history, meta_instruction):
def chat(self, tokenizer, query, image, history, streamer, max_new_tokens, do_sample, temperature, top_p, repetition_penalty, meta_instruction):
def stream_chat(self, tokenizer, query, history, max_new_tokens, do_sample, temperature, top_p):
class ChatStreamer(BaseStreamer):
def stream_producer():
def consumer():
def __init__(self, tokenizer):
def put(self, value):
def end(self):
